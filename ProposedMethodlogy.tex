\documentclass[12pt,a4paper]{report}
\usepackage[left=1.57in,top=1.18in,bottom=1in,right=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{csquotes}
\usepackage{float}
\usepackage{color}
\usepackage{gensymb}
\usepackage{multirow}
\usepackage{array}
\usepackage[toc,page]{appendix}
\usepackage{setspace}
\singlespacing
\captionsetup[figure]{labelfont={bf},labelformat={default},labelsep=period,name={Fig.}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%% Title-Pages %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{Title-Pages.tex}

%%%%%%%%%% Acknowledgement %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{Acknowledgement.tex}

%%%%%%%%%% Abstract %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{Abstract.tex}

%%%%%%%%% List of Different Contents %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents
\listoffigures
\listoftables
\clearpage
\pagenumbering{arabic}

\onehalfspacing  %%%% One Half spacing  start from here %%%%%
%%%%%% Chapter 1 Introduction %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{Introduction.tex}

%%%%%%%%%% Chapter 2 Literature Review %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{LiteratureReview.tex}
    
%%%%%%%%% Chapter 3 Proposed Methodology %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Proposed Methodology}
In this chapter we will discuss about our proposed methodology and learn about every module of our system. We will find short mathematical insight of each learning algorithm used in our system. Finally overall system is summarized with an example.
\section{Proposed Text Classification System}
The key objective of our project is to design a system that can classify suspicious and non-suspicious text. \textbf{Fig} \ref{fig:proposed_model} shows an abstract view of our system.
\vspace{0.5cm}
\begin{figure}[h!]
\centering
  \includegraphics[scale=0.6]{Figures/proposed_model.PNG}
  \caption{Proposed Model of Suspicious Text Detection}
  \label{fig:proposed_model}
\end{figure}
 
 \subsection{Training Text Corpus}
 Supervised learning involves using a set of training examples that make up the training data. In linguistics, a corpus or text corpus is a large and structured set of texts nowadays usually electronically stored and processed. For a machine learning algorithm a well corpus is essential to perform according to expectation. In the corpus each training example consists of an input text and desired output value. An algorithm is then applied to the data to produce a classifier which will determine the correct output for any further valid input values. In Bengali language processing there is no corpus available which contains suspicious text. But as ours is a supervised classification system, our suspicious text detector needs a training corpus which consist of text and labels indicating whether a text is suspicious or non-suspicious. To implement this system we collect a large amount of Bangla  suspicious text. This texts are collected from online blogs, newspaper, Facebook post and other type of electronic source. Training corpus is divided into positive set (Suspicious text) and negative set (non-suspicious text). The accuracy of the learning algorithms depend on uniqueness of training examples.
 
 \subsection{Preprocessing}
 In natural language processing, data preprocessing is an essential step. Data preprocessing is a data mining technique that involves transforming raw data into an understandable format. Real world data is often incomplete, inconsistent, and/or lacking in certain behaviors or trends, and is likely to contain many errors. Data preprocessing is a proven method of resolving such issues. Data preprocessing is required to fill in missing values, smooth noisy data, identify or remove the outliers, and resolve inconsistencies.
 Words with no significance must be removed from the text. In our system all texts are preprocessed by the preprocessor. We use main bodies of the text to train suspicious text detector. We are going to represent our text document by list of words and their frequencies. We have a stop word list which consist of words that make no contribution to classify text. In our preprocessing section, such word will be removed by matching with stop word list. It will be very helpful to increase efficiency of the system because redundant will slow our system and increase computational complexity of the system.
 
\subsection{Feature Extraction}
Word frequencies will be used as feature in our proposed system. Word frequencies are used as features quite often, and although they are usually considered a basic feature, they can prove to be effective.
Terms associated with feature extraction of our system are described shortly.
\subsubsection{CountVectorizer}

CountVectorizers used to learn the vocabulary of a set of texts and then transform them into a data-frame that can be used for building models. CountVectorizer takes few parameter that is important for extracting feature accurately.

\subparagraph{Stop Words :}
CountVectorizer just counts the occurrences of each word in its vocabulary, extremely common words stop words will become very important features while they add little meaning to the text. In our system we do not take those words into account which improves system accuracy.
\subparagraph{Min-DF, Max-DF :}
These parameters are the minimum and maximum document frequencies words/n-grams must have to be used as features. If either of these parameters are set to integers, they will be used as bounds on the number of documents each feature must be in to be considered as a feature. If either is set to a float, that number will be interpreted as a frequency rather than a numerical limit.
\subparagraph{Max-Features :}
\texttt{max\_features} parameter is used to limit maximum number of feature used by the model. In our system we take most frequent 1000 words by using this parameter to reduce time and storage complexity.
\subsubsection{Word2vec}
Word2vec is used to train different learning model. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space. Accuracy increases overall as the number of words used increases, and as the number of dimensions increases. But we should be careful about associated complexities. 

\subsubsection{TF-IDF}


\end{document}









