\documentclass[12pt,a4paper]{report}
\usepackage[left=1.57in,top=1.18in,bottom=1in,right=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{csquotes}
\usepackage{float}
\usepackage{color}
\usepackage{gensymb}
\usepackage{multirow}
\usepackage{array}
\usepackage[toc,page]{appendix}
\usepackage{setspace}
\singlespacing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%% Title-Pages %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{Title-Pages.tex}

%%%%%%%%%% Acknowledgement %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{Acknowledgement.tex}

%%%%%%%%%% Abstract %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{Abstract.tex}

%%%%%%%%% List of Different Contents %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents
\listoffigures
\listoftables
\clearpage
\pagenumbering{arabic}

%%%%%% Chapter 1 Introduction %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\onehalfspacing
\input{Introduction.tex}

%%%%%%%%%% Chapter 2 Literature Review %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Literature Review}
\thispagestyle{empty}
In this chapter we will shortly describe history of text classification and learn about different machine learning methods which are really useful for classifying text.This chapter also contains brief discussion on related previous works.

\section{History of Text Classification}
Text classification (or text categorization)  first emerged in the late 1990s as ``Text Data Mining" and has been actively investigated by many researchers. Text classification is a preprocessing technology which can be used to filter out irrelevant documents from a large-scale corpus. In early approaches a text source is treated as bag-of-words. The bag-of-words is a simple representation in which a sentence or a document is expressed by a set of words and it is used in natural language processing. But there was no ability to understand the semantics of a document in earlier approaches.\par
\vspace{.5cm}
After early ages, researchers try to find out hidden relationships and other complex pattern within data-sets. Techniques such as clustering, classification, decision trees and link analysis are used to find out these complex relational patterns. This techniques coupled with machine learning algorithms helps to find deeper linguistics that enable to understand the semantics of a document or a sentence. Text classification helps to allocate documents into predefined topics, such as economics, politics, news, sports etc. Due to the excessive increase in online textual information, e.g., email messages, online news, web pages, as well as a huge number of resources for scientific online abstracts such as MEDLINE, there is an ever-growing demand for text classification. It has now become one of the most attractive research topic. If a system can classify texts accurately we can use it to predict events which will happen in future from our present data. Such as from suspicious communication we may predict a terrorist event. It is an interesting question how to achieve high performance in the task of assigning multiple topics to documents in a targeted domain as the system has to process huge amount of data. If we can develop system with high information processing capacity there will be revolution because this an era of information.


\section{Different Methods of Text Classification}
Many works have already done in text categorization such as spam classification, research paper categorization, detecting suspicious profile using text classification etc.There are several well-known methods and algorithms are already exist for text classification problem such as Na√Øve Bayes Classifier, Support Vector Machine, Decision Tree, Logistic regression, K-nearest neighbor algorithms (KNN) etc.

\subsection{Naive Bayes Classifier}

Naive Bayes is a probabilistic classifier commonly used in machine learning. The Bayesian classifiers are statistical and also possess learning ability. For processing large data-set multinomial model of Naive Bayes is used. By searching the dependencies among attributes the performance of Naive Bayes could be enhanced. It is mainly used in data preprocessing applications due to ease of computation. Bayesian reasoning and probability inference are employed in predicting the target class. Attributes are key in classification using probabilistic model. So weight values of attributes play an important role to improve the performance of the model. %Deep feature weighting solves the problem of conditional independence assumption, which is a major improvement of Naive Bayesian classifier and computes conditional probability accurately. But these feature weighting techniques come with some defects like, inadequate improvement to performance, compromised simplicity and increased execution time of models etc. 
\par
\vspace{0.5cm}
The performance of Naive Bayes depends on the accuracy of the estimated conditional probability terms. When training data is scatter it is hard to estimate conditional probability terms accurately. Some meta learning methods are followed to estimate conditional probability terms. To improve this, various meta-learning techniques such as structure extension, attribute selection, frequency transforming, attribute weighting, instance weighting, and local learning are used. Naive Bayes classifier is applied to mark email as spam/ham, classify articles based on content,sentiment/emotion analysis.% In our project it can also be used classify texts into suspicious and non-suspicious category.
The advantage of Naive Bayesian classifiers are these classifiers are simple and  powerful in terms of degree of certainty, optimization is less complicated and allows for dynamic adaption.These qualities  make them an easier option for handling natural language processing problems. \par
\vspace{.5cm}
The main limitation of Bayesian networks is that the time complexity increases when high dimensional text data is processed using these networks. Moreover, in Bayesian networks interaction between features can not be achieved and the probabilities calculated are not accurate but relative probability.

\subsection{Support Vector Machine}
The Support Vector Machine (SVM) algorithm is a supervised machine learning algorithm which is used for various classification problems.It can be applied in credit risk analysis, medical diagnosis, text categorization, and information extraction.SVM is suitable for high dimensional data because the complexity of the classifiers depends on the number
of support vectors instead of data dimensions, they produce the same hyper plane for repeated training sets, and they have better generalization abilities. It separate classes by placing hyper plane between classes. It selects optimal hyper plane from which distance of classes are maximized. The performance of SVM does not decrease with sparsity of data. SVM is a really powerful tool for processing data and extracting information when dataset in huge.
\par 
\vspace{0.5cm}
The performance of SVM can be enhanced using customized kernels. One such customized kernel is Class Meaning Kernel that is used to smooth terms of documents using class based meaning values of terms. SVM has some salient features for which it has been considered as state of art in the classification tasks. SVM has been used for text classification, hand written digit detection and many other classification tasks. Some of its unique features are: it can work well in a very high dimensional feature space, it uses only a subset of original training set to make decision boundary called support vectors and it is also suitable for non linearly separable data (it uses kernel trick). In SVM we can select maximum features length for our model during learning. In our SVM learning model we took most frequent 1000 features using the parameter max-features.  
\par 
\vspace{0.5cm}
The limitation of Support Vector Machine is it still lags in handling unlabeled data. It has to be verified that data is thoroughly preprocessed to increase the performance of classifiers. In addition, selecting the best kernel among available kernels to train data is time consuming. Training and testing using SVM model is time consuming. As SVM is non parametric model it could not summarize data based on underlying parameters. 
\clearpage

\subsection{Logistic Regression}
Logistic regression is a technique borrowed by machine learning from the field of statistics. It is the go-to method for binary classification problems (problems with two class values).

\end{document}









